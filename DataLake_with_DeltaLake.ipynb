{"cells":[{"cell_type":"markdown","source":["Building Reliable Data Lakes with Delta Lake and Apache Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7ee022e3-6c90-479a-aa71-00c78bf949d8"}}},{"cell_type":"code","source":["\"\"\"\nOpen format: Stored as Parquet format in blob storage.\nACID Transactions: Ensures data integrity and read consistency with complex, concurrent data pipelines.\nSchema Enforcement and Evolution: Ensures data cleanliness by blocking writes with unexpected.\nAudit History: History of all the operations that happened in the table.\nTime Travel: Query previous versions of the table by time or version number.\nDeletes and upserts: Supports deleting and upserting into tables with programmatic APIs.\nScalable Metadata management: Able to handle millions of files are scaling the metadata operations with Spark.\nUnified Batch and Streaming Source and Sink: A table in Delta Lake is both a batch table, as well as a streaming source and sink. Streaming data ingest, batch historic backfill, and interactive queries all just work out of the box.\nSetup Instructions\nYou need DBR 7.6 or above.\n\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1d7e5575-2f26-4c76-8d68-63baa599e477"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["spark.sql(\"set spark.sql.shuffle.partitions = 1\")\n \nsourcePath = \"/databricks-datasets/learning-spark-v2/loans/loan-risks.snappy.parquet\"\n \n# Configure Delta Lake Path\ndeltaPath = \"/tmp/loans_delta\"\n \n# Remove folder if it exists\ndbutils.fs.rm(deltaPath, recurse=True)\n \n# Create the Delta table with the same loans data\n(spark.read.format(\"parquet\").load(sourcePath) \n  .write.format(\"delta\").save(deltaPath))\n \nspark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")\nprint(\"Defined view 'loans_delta'\")\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebd90d2c-2931-4679-ac19-710797d1a57c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Defined view &#39;loans_delta&#39;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Defined view &#39;loans_delta&#39;\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["d Let's explore the data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"01c68182-68fe-4ca4-b7ed-5dea739023d8"}}},{"cell_type":"code","source":["spark.sql(\"SELECT count(*) FROM loans_delta\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b50f8b06-5209-4517-99db-bc259d0c4545"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"SELECT * FROM loans_delta LIMIT 5\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"505b2ea4-8938-4e0e-94b9-ce270edab21c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Loading data streams into Delta Lake table\n\nWe will generate a stream of data from with randomly generated loan ids and amounts. In addition, we are going to define a few useful utility functions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f6d4a50-45cd-477e-abff-1a8893986b7c"}}},{"cell_type":"code","source":["import random\nimport os\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n \ndef random_checkpoint_dir(): \n  return \"/tmp/chkpt/%s\" % str(random.randint(0, 10000))\n \n \n# User-defined function to generate random state\n \nstates = [\"CA\", \"TX\", \"NY\", \"WA\"]\n \n@udf(returnType=StringType())\ndef random_state():\n  return str(random.choice(states))\n \n \n# Function to start a streaming query with a stream of randomly generated data and append to the parquet table\ndef generate_and_append_data_stream():\n \n  newLoanStreamDF = (spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 5).load() \n    .withColumn(\"loan_id\", 10000 + col(\"value\")) \n    .withColumn(\"funded_amnt\", (rand() * 5000 + 5000).cast(\"integer\")) \n    .withColumn(\"paid_amnt\", col(\"funded_amnt\") - (rand() * 2000)) \n    .withColumn(\"addr_state\", random_state())\n    .select(\"loan_id\", \"funded_amnt\", \"paid_amnt\", \"addr_state\"))\n    \n  checkpointDir = random_checkpoint_dir()\n \n  streamingQuery = (newLoanStreamDF.writeStream \n    .format(\"delta\") \n    .option(\"checkpointLocation\", random_checkpoint_dir()) \n    .trigger(processingTime = \"10 seconds\") \n    .start(deltaPath))\n \n  return streamingQuery\n \n# Function to stop all streaming queries \ndef stop_all_streams():\n  # Stop all the streams\n  print(\"Stopping all streams\")\n  for s in spark.streams.active:\n    s.stop()\n  print(\"Stopped all streams\")\n  print(\"Deleting checkpoints\")  \n  dbutils.fs.rm(\"/tmp/chkpt/\", True)\n  print(\"Deleted checkpoints\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cbe1f3c-1e52-4951-906f-73edd943e5b5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["streamingQuery = generate_and_append_data_stream()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6190cb6-3f3b-4f9d-a45c-fadf3a4a68e7"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can see that the streaming query is adding data to the table by counting the number of records in the table. Run the following cell multiple times."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b9779587-fb21-4a58-80a6-6df2b6069e4e"}}},{"cell_type":"code","source":["display(spark.sql(\"SELECT count(*) FROM loans_delta\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd2d7f13-ba5f-4efe-b59f-4f6526d81cc8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Remember to stop all the streaming queries."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78ec5f93-664a-4b59-8f3f-17aab9399285"}}},{"cell_type":"code","source":["stop_all_streams()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8537afe8-b0cc-4903-8b11-41fb41b20eba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Enforcing schema on write to prevent data corruption\n\nLet’s test this by trying to write some data with an additional column closed that signifies whether the loan has been terminated. Note that this column does not exist in the table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a32b965-794f-4d9e-a669-254ff1b68578"}}},{"cell_type":"code","source":["cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n \nitems = [\n  (1111111, 1000, 1000.0, 'TX', True), \n  (2222222, 2000, 0.0, 'CA', False)\n]\n \nfrom pyspark.sql.functions import *\n \nloanUpdates = (spark\n                .createDataFrame(items, cols)\n                .withColumn(\"funded_amnt\", col(\"funded_amnt\").cast(\"int\")))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"873c91e5-97f7-4032-a38b-5f26e83bd6a8"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Evolving schema to accommodate changing data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae1dd063-97c3-4451-aeb7-bf1d7df62a16"}}},{"cell_type":"code","source":["(loanUpdates.write.format(\"delta\").mode(\"append\")\n  .option(\"mergeSchema\", \"true\")\n  .save(deltaPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"485c2fe3-d746-4344-8191-065af543c633"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's query the table once again to see the schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6702d0f3-9ecc-4a23-b864-fd1b0c418d71"}}},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(deltaPath).filter(\"loan_id = 1111111\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4e97ee2-5b72-4bd8-af90-141957bc8905"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["For existing rows are read, the value of the new column is considered as NULL."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c47c3004-cdcf-401f-bac4-848c80689d22"}}},{"cell_type":"code","source":["display(spark.read.format(\"delta\").load(deltaPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e450d923-3926-445f-8898-fefa5d035466"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Transforming existing data\n\nLet's look into how we can transform existing data. But first, let's refine the view on the table because the schema has changed and the view needs to redefined to pick up the new schema."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"834c4563-c56e-4031-970b-88f52f2e869f"}}},{"cell_type":"code","source":["spark.read.format(\"delta\").load(deltaPath).createOrReplaceTempView(\"loans_delta\")\nprint(\"Defined view 'loans_delta'\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b299da07-676b-43de-8952-4491a0f90492"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"116eebf1-6505-43ce-8f1e-a20f3e490edf"}}},{"cell_type":"code","source":["\"\"\"\n        Updating loan data to fix errors\n\nUpon reviewing the data, we realized that all of the loans assigned to addr_state = 'OR' should have been assigned to addr_state = 'WA'.\nIn Parquet, to do an update, you would need to\nCopy all of the rows that are not affected into a new table\nCopy all of the rows that are affected into a DataFrame, perform the data modification\nInsert the previously noted DataFrame's rows into the new table\nRemove the old table\nRename the new table to the old table\n\"\"\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"358e4532-3321-4e32-946b-bfe04395f9dd"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[3]: &#34;\\n        Updating loan data to fix errors\\n\\nUpon reviewing the data, we realized that all of the loans assigned to addr_state = &#39;OR&#39; should have been assigned to addr_state = &#39;WA&#39;.\\nIn Parquet, to do an update, you would need to\\nCopy all of the rows that are not affected into a new table\\nCopy all of the rows that are affected into a DataFrame, perform the data modification\\nInsert the previously noted DataFrame&#39;s rows into the new table\\nRemove the old table\\nRename the new table to the old table\\n&#34;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[3]: &#34;\\n        Updating loan data to fix errors\\n\\nUpon reviewing the data, we realized that all of the loans assigned to addr_state = &#39;OR&#39; should have been assigned to addr_state = &#39;WA&#39;.\\nIn Parquet, to do an update, you would need to\\nCopy all of the rows that are not affected into a new table\\nCopy all of the rows that are affected into a DataFrame, perform the data modification\\nInsert the previously noted DataFrame&#39;s rows into the new table\\nRemove the old table\\nRename the new table to the old table\\n&#34;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"\"\"SELECT addr_state, count(1) FROM loans_delta WHERE addr_state IN ('OR', 'WA', 'CA', 'TX', 'NY') GROUP BY addr_state\"\"\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c3a7aa8c-6efd-48a8-88f0-81a370c6fc8a"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's fix the data."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"141f5cce-aa3a-4047-b155-a08542a8f771"}}},{"cell_type":"code","source":["from delta.tables import *\n \ndeltaTable = DeltaTable.forPath(spark, deltaPath)\ndeltaTable.update(\"addr_state = 'OR'\",  {\"addr_state\": \"'WA'\"})"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"546922d9-2d7f-41b7-af92-937ab74e7f48"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's see the data once again."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"296d0d1f-6316-44d2-ad97-5a111eb67ef5"}}},{"cell_type":"code","source":["display(spark.sql(\"\"\"SELECT addr_state, count(1) FROM loans_delta WHERE addr_state IN ('OR', 'WA', 'CA', 'TX', 'NY') GROUP BY addr_state\"\"\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"700665dd-a191-4c81-8204-2bd740d9ac37"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Deleting user-related data from a table General Data Protection Regulation (GDPR)\n\nYou can remove data that matches a predicate from a Delta Lake table. Let's say we want to remove all the fully paid loans. Let's first see how many are there."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"793706d5-1ff4-4a5c-a8c6-40445224d6ca"}}},{"cell_type":"code","source":["display(spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b643cb7b-13a2-4e75-acc3-683d8b6e9526"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Now let's delete them."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"546db947-95b6-4d7a-bd44-e548ad3e1065"}}},{"cell_type":"code","source":["from delta.tables import *\n \ndeltaTable = DeltaTable.forPath(spark, deltaPath)\ndeltaTable.delete(\"funded_amnt = paid_amnt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82f17f4d-52bb-42be-8873-f8404f9b10ba"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's check the number of fully paid loans once again."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4482d8b6-5b67-4dfd-82a5-26b81c6d0c8c"}}},{"cell_type":"code","source":["display(spark.sql(\"SELECT COUNT(*) FROM loans_delta WHERE funded_amnt = paid_amnt\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"05a41c3b-1005-47ab-8236-7b49dfa33f33"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Upserting change data to a table using merge\nA common use cases is Change Data Capture (CDC), where you have to replicate row changes made in an OLTP table to another table for OLAP workloads. To continue with our loan data example, say we have another table of new loan information, some of which are new loans and others are updates to existing loans. In addition, let’s say this changes table has the same schema as the loan_delta table. You can upsert these changes into the table using the DeltaTable.merge() operation which is based on the MERGE SQL command."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"92397ab8-52d1-4742-9da2-83568c7cad4e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dc7e3552-cb60-4c1e-883e-9f66a3d17e24"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's say we have some changes to this data, one loan has been paid off, and another new loan has been added."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ef728121-7128-4a69-ace4-ec5a4c9038ae"}}},{"cell_type":"code","source":["cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n \nitems = [\n  (11, 1000, 1000.0, 'NY', True),   # loan paid off\n  (12, 1000, 0.0, 'NY', False)      # new loan\n]\nloanUpdates = spark.createDataFrame(items, cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97adf2f6-a0ce-4f3b-8f47-a4318e9449b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Now, let's update the table with the change data using the merge operation."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c01b4cea-f8c5-4908-863f-c28488560647"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from delta.tables import *\n \ndeltaTable = DeltaTable.forPath(spark, deltaPath)\n \n(deltaTable\n  .alias(\"t\")\n  .merge(loanUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\") \n  .whenMatchedUpdateAll() \n  .whenNotMatchedInsertAll() \n  .execute())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dacaec22-6ef4-46ec-9617-eaab1b418d61"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Let's see whether the table has been updated."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"035f817d-79f9-4b03-a94e-4bf4cc02b7fc"}}},{"cell_type":"code","source":["display(spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4c6c46f4-8b09-4d2f-8f46-16bdc4f11a2b"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Deduplicating data while inserting using insert-only merge\nThe merge operation in Delta Lake supports an extended syntax beyond that specified by the ANSI standard. It supports advanced features like the following.\n\nDelete actions: For example, MERGE … WHEN MATCHED THEN DELETE\nClause conditions: For example, `MERGE … WHEN MATCHED AND THEN ...``\nOptional actions: All the MATCHED and NOT MATCHED clauses are optional.\nStar syntax: For example, UPDATE * and INSERT * to update/insert all the columns in the target table with matching columns from the source dataset. The equivalent API in DeltaTable is updateAll() and insertAll(), which we have already seen.\nThis allows you to express many more complex use cases with little code. For example, say you want to backfill the loan_delta table with historical data of past loans. But some of the historical data may already have been inserted in the table and you don't want to update them (since their emails may already have been updated). You can deduplicate by the loan_id while inserting by running the following merge operation with only the INSERT action (since the UPDATE action is optional)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"55757ad6-97d3-4ef8-a671-b7f5d6fee8f9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c13bd461-dfa6-4f84-82db-899de4473b53"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Let's say we have some historical data that we want to merge with this table. One of the historical loan exists in the current table but the historical table has old values, therefore it should not update the current value present in the table. And another historical does not exist in the current table, therefore it should be inserted into the table."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"320cf55a-82da-44eb-a9f5-72d494c787fc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["cols = ['loan_id', 'funded_amnt', 'paid_amnt', 'addr_state', 'closed']\n \nitems = [\n  (11, 1000, 0.0, 'NY', False),\n  (-100, 1000, 10.0, 'NY', False)\n]\n \nhistoricalUpdates = spark.createDataFrame(items, cols)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5e4eae67-7da6-444f-aa23-f3a2a520de27"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Let's do the merge."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"612a6a57-7cd1-44b3-8029-ac8766fb1eed"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from delta.tables import *\n \ndeltaTable = DeltaTable.forPath(spark, deltaPath)\n \n(deltaTable\n  .alias(\"t\")\n  .merge(historicalUpdates.alias(\"s\"), \"t.loan_id = s.loan_id\") \n  .whenNotMatchedInsertAll() \n  .execute())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9a8d2a4-9fc6-4134-981b-976f4a46f880"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Let's see whether the table has been updated."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bf2b2c31-bdd4-4e8c-83ef-5ecba4c20bfc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(spark.sql(\"select * from loans_delta where addr_state = 'NY' and loan_id < 30\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd74bed5-6869-4bac-8dd4-c6f5dd79482f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Notice that the only change in the table is that insert of new loan, and existing loans were not updated to old values."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e54e0469-c962-42fb-ba6a-2d7d9296be3f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Auditing data changes with operation history\nAll changes to the Delta table are recorded as commits in the table's transaction log. As you write into a Delta table or directory, every operation is automatically versioned. You can use the HISTORY command to view the table's history."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"82091414-44f4-4716-810b-d0d253fe1fb5"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from delta.tables import *\n \ndeltaTable = DeltaTable.forPath(spark, deltaPath)\ndisplay(deltaTable.history())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc7e9a0e-191b-45ac-a4a8-8dd0b9e305cc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(deltaTable.history(4).select(\"version\", \"timestamp\", \"operation\", \"operationParameters\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b354194d-6ab0-4d36-a619-dfa50bb1d200"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["Querying previous snapshots of the table with time travel\nDelta Lake’s time travel feature allows you to access previous versions of the table. Here are some possible uses of this feature:\n\nAuditing Data Changes\nReproducing experiments & reports\nRollbacks\nYou can query by using either a timestamp or a version number using Python, Scala, and/or SQL syntax. For this examples we will query a specific version using the Python syntax.\n\nFor more information, refer to Introducing Delta Time Travel for Large Scale Data Lakes and the docs.\n\nLet's query the table's state before we deleted the data, which still contains the fully paid loans."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"98d8c4ae-887d-468e-8fcd-f04b103d670e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["previousVersion = deltaTable.history(1).select(\"version\").first()[0] - 3\n \n(spark.read.format(\"delta\")\n  .option(\"versionAsOf\", previousVersion)\n  .load(deltaPath)\n  .createOrReplaceTempView(\"loans_delta_pre_delete\"))\n \ndisplay(spark.sql(\"SELECT COUNT(*) FROM loans_delta_pre_delete WHERE funded_amnt = paid_amnt\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4a58c3b-9fc4-47b0-a352-844a05f08ab2"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DataLake_with_DeltaLake","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2652602799820534}},"nbformat":4,"nbformat_minor":0}
